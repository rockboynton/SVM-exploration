{
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# State Vector Machines\n",
    "\n",
    "## Introduction\n",
    "\n",
    "## Summary of Results\n",
    "\n",
    "---\n",
    "\n",
    "## Setup \n",
    "\n",
    "First, we'll need to import the various libraries that we'll need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import Imputer\n",
    "from sklearn.impute import SimpleImputer\n",
    "# from sklearn_pandas import CategoricalImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load and Clean Data\n",
    "\n",
    "Before we can build any models, we need to import the data and clean it by converting types as necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Adult/adult.data\", names=[\n",
    "    \"age\",\n",
    "    \"workclass\",\n",
    "    \"fnlwgt\",\n",
    "    \"education\",\n",
    "    \"education_num\",\n",
    "    \"marital_status\",\n",
    "    \"occupation\",\n",
    "    \"relationship\",\n",
    "    \"race\",\n",
    "    \"sex\",\n",
    "    \"capital_gain\",\n",
    "    \"capital_loss\",\n",
    "    \"hours_per_week\",\n",
    "    \"native_country\",\n",
    "    \"earning_label\"\n",
    "], skipinitialspace=True)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore some attributes of the dataset\n",
    "\n",
    "# dir(df)\n",
    "print(\"Features: \", df.columns)\n",
    "print(\"Labels: \", pd.Series.unique(df.earning_label))\n",
    "print(\"Shape: \", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_names = pd.Series.unique(df.earning_label)\n",
    "# feature_names = df.columns\n",
    "\n",
    "# ranges = []\n",
    "# type_count = 0\n",
    "# for i in range(0, len(target_names) - 1):\n",
    "#     additional = np.count_nonzero(labels == i)\n",
    "#     ranges.append((i, type_count, additional + type_count))\n",
    "#     type_count += additional\n",
    "    \n",
    "# for i in range(0, 4)[0:4]:\n",
    "#     for j in range(i + 1, 4):\n",
    "#         for target_type, start, end in ranges:\n",
    "#             plot.scatter(data[start:end, i], data[start:end ,j], label=target_names[target_type])\n",
    "#         plot.xlabel(dataset.feature_names[i])\n",
    "#         plot.ylabel(dataset.feature_names[j])\n",
    "#         plot.legend()\n",
    "#         plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because the native.country variable has too many categories, and most of the data points are from the US (91%), we combine all the categories except for “United-States”into the “Other” category:\n",
    "df.loc[df['native_country']!='United-States', 'native_country']='Other'\n",
    "\n",
    "# Save the output label in binary encoding, 0: <=50k, 1: > 50k\n",
    "Y=pd.Categorical(df['earning_label']).codes\n",
    "Y = np.where(Y==0, -1, Y) \n",
    "print(Y)\n",
    "\n",
    "# Education is not needed as uducation_num performs its function\n",
    "# Also drop the label as it is not needed for the model\n",
    "df=df.drop(['education','earning_label'], axis=1)\n",
    "\n",
    "# Scale numerical features\n",
    "col_names = ['age', 'fnlwgt', 'education_num', 'capital_gain', 'capital_loss','hours_per_week']\n",
    "features = df[col_names]\n",
    "scaler = StandardScaler().fit(features.values)\n",
    "features = scaler.transform(features.values)\n",
    "df[col_names] = features\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine unique values of each categorical feature:\n",
    "col_names = ['workclass','marital_status','occupation','relationship','race','sex','native_country']\n",
    "for feature in col_names:\n",
    "    print(feature, pd.Series.unique(df[feature]))\n",
    "\n",
    "# impute missing values\n",
    "features = df[col_names]\n",
    "imp = SimpleImputer(strategy='most_frequent').fit(features.values)\n",
    "features = imp.transform(features.values)\n",
    "df[col_names] = features\n",
    "\n",
    "# Convert categorical features to one-hot encoding\n",
    "df=pd.get_dummies(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, split the data into training and testing sets 80\\/20\n",
    "train_X, test_X, train_Y, test_Y = train_test_split(df.values, Y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fitting the Model\n",
    "\n",
    "With the data now processed, it is ready to have SVM applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from svm import SVM\n",
    "svm = SVM(10, .001)\n",
    "w = svm.fit(train_X, train_Y)\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_Y = svm.predict(test_X, w)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(\"Accuracy:\")\n",
    "print ((predicted_Y - test_Y == 0).sum() / test_Y.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "clf = SVC(gamma='auto')\n",
    "clf.fit(train_X, train_Y)\n",
    "SVC(gamma='auto')\n",
    "print(clf.predict(test_X))\n",
    "\n",
    "print(df.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_Y = clf.predict(test_X)\n",
    "print(predicted_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy:\")\n",
    "print ((predicted_Y - test_Y == 0).sum() / test_Y.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Acknowlegements\n",
    "\n",
    "https://methods.sagepub.com/dataset/howtoguide/support-vector-machine-in-aci-1996-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3.6.1 64-bit ('root': conda)",
   "language": "python",
   "name": "python36164bitrootconda6e0c314544c443b5938248d6fa4e1245"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1-final"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}